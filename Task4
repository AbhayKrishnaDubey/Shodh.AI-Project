1. Present Your Results

Deep Learning (DL) Model – Evaluation Metrics
For the DL model, we evaluate using AUC and F1-Score.

| Metric       | Meaning                                                         | Value         |
| ------------ | --------------------------------------------------------------- | ------------- |
| AUC          | Measures how well the model separates good vs bad borrowers     | Example: 0.85 |
| F1-Score     | Balance between catching good customers and avoiding risky ones | Example: 0.62 |

Your RL evaluation produced the following:

| Policy        | Average Reward per Decision |
| ------------- | --------------------------- |
| PPO Agent     | -0.082187                   |
| Approve All   | -4.015352                   |
| Random Policy | -2.005484                   |


Interpretation:
A higher (less negative) reward indicates a better lending strategy.
The RL agent performs significantly better than the baselines because -0.08 is much closer to zero than -4 or -2.






2. Why These Metrics Are Used

Why AUC and F1-Score for the DL Model
AUC: AUC shows how well the model separates safe borrowers from risky borrowers.
     A higher AUC indicates better ranking ability.

F1-Score:- It evaluates the model at a particular decision threshold (for example, approving if probability > 0.5).
           It balances:
            Precision: how many approved loans were actually good
            Recall: how many good customers the model successfully identified

Together, AUC and F1 give:
Overall prediction quality across all thresholds (AUC)
Decision quality at a chosen threshold (F1)

Why Estimated Policy Value for the RL Agent?

The RL agent is not predicting default probability.
Instead, it learns which action leads to higher long-term profit.

Estimated Policy Value represents:

Expected gain or loss from following the agent’s decisions

The real business objective: maximize profit and minimize losses

Therefore, this metric directly measures policy performance and is the correct evaluation method for RL.






3. Comparing the DL Policy and the RL Policy

DL Model’s Policy:- 

The DL model outputs a default probability.
A simple decision rule looks like:

If predicted default probability < 0.5, then approve
Otherwise, deny
This makes the DL model risk-focused.

RL Agent’s Policy

The RL agent chooses actions based on maximizing reward (profit).
It may approve some risky applicants if:
Loan amount is high
Expected repayment profit is high
The expected benefit outweighs the potential loss
This makes the RL model profit-focused.


Example Scenario Where They Disagree
| Feature                            | Value            |
| ---------------------------------- | ---------------- |
| Predicted default probability (DL) | 0.65 (DL denies) |
| Loan amount                        | 25,000           |
| Expected payment                   | 30,000           |
| RL decision                        | Approve          |


Although the borrower is risky, the potential profit is high.
The RL agent learns that approving such loans can increase overall portfolio reward.

Core Difference

  DL Model: Prioritizes safety.
  RL Model: Prioritizes profit.

This difference naturally leads to disagreement on some applicants.






4. Future Steps

--> Deployment Readiness

    1. The models should not be deployed yet.
    2. Further steps needed:
    3. Testing on real historical loan data
    4. Fairness evaluation
    5. Stability checks over time
    6. Better calibration of the reward function to real monetary values

--> Limitations of the Current Approach

    1. RL environment is simplified
    2. Reward design can strongly influence behavior
    3. No real bank data used
    4. DL model uses limited features
    5. RL policy is harder to explain to auditors

--> Additional Data to Collect

    1. Income and employment details
    2. Bank transaction behavior
    3. More detailed loan history
    4. Economic indicators
    5. Past repayment behavior such as late payments

--> Other Algorithms to Explore

    1. XGBoost or LightGBM for tabular data
    2. Constrained Reinforcement Learning
    3. Offline Reinforcement Learning using logged bank data
    4. Contextual Bandit algorithms
